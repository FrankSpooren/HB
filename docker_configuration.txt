# ==========================================
# DOCKER-COMPOSE.YML - Main Orchestration
# ==========================================
version: '3.8'

services:
  # MongoDB Database
  mongodb:
    image: mongo:7.0
    container_name: holidaibutler-mongodb
    restart: unless-stopped
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ROOT_USERNAME:-admin}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD:-password123}
      MONGO_INITDB_DATABASE: ${MONGO_DB_NAME:-holidaibutler}
    volumes:
      - mongodb_data:/data/db
      - ./scripts/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    networks:
      - holidaibutler-network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redis Cache
  redis:
    image: redis:7.2-alpine
    container_name: holidaibutler-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis123}
    volumes:
      - redis_data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - holidaibutler-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Backend API Server
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: development
    container_name: holidaibutler-backend
    restart: unless-stopped
    ports:
      - "3000:3000"
      - "9229:9229" # Debug port
    environment:
      NODE_ENV: development
      PORT: 3000
      MONGODB_URI: mongodb://${MONGO_ROOT_USERNAME:-admin}:${MONGO_ROOT_PASSWORD:-password123}@mongodb:27017/${MONGO_DB_NAME:-holidaibutler}?authSource=admin
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis123}@redis:6379
      JWT_SECRET: ${JWT_SECRET:-your-super-secret-jwt-key-here}
      CLAUDE_API_KEY: ${CLAUDE_API_KEY}
      STRIPE_SECRET_KEY: ${STRIPE_SECRET_KEY}
      EMAIL_HOST: ${EMAIL_HOST}
      EMAIL_PORT: ${EMAIL_PORT}
      EMAIL_USER: ${EMAIL_USER}
      EMAIL_PASS: ${EMAIL_PASS}
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET}
      APPLE_CLIENT_ID: ${APPLE_CLIENT_ID}
      APPLE_CLIENT_SECRET: ${APPLE_CLIENT_SECRET}
      CLOUDINARY_URL: ${CLOUDINARY_URL}
      TWILIO_ACCOUNT_SID: ${TWILIO_ACCOUNT_SID}
      TWILIO_AUTH_TOKEN: ${TWILIO_AUTH_TOKEN}
    volumes:
      - ./backend:/app
      - /app/node_modules
      - backend_uploads:/app/uploads
      - backend_logs:/app/logs
    depends_on:
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - holidaibutler-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Nginx Reverse Proxy
  nginx:
    image: nginx:1.25-alpine
    container_name: holidaibutler-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/api.conf:/etc/nginx/conf.d/default.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      - backend
    networks:
      - holidaibutler-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: holidaibutler-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - holidaibutler-network

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:10.1.0
    container_name: holidaibutler-grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin123}
      GF_USERS_ALLOW_SIGN_UP: false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
    networks:
      - holidaibutler-network

  # Elasticsearch (for logging)
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
    container_name: holidaibutler-elasticsearch
    restart: unless-stopped
    ports:
      - "9200:9200"
    environment:
      discovery.type: single-node
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      xpack.security.enabled: false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - holidaibutler-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kibana (for log visualization)
  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.0
    container_name: holidaibutler-kibana
    restart: unless-stopped
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - holidaibutler-network

  # Logstash (for log processing)
  logstash:
    image: docker.elastic.co/logstash/logstash:8.10.0
    container_name: holidaibutler-logstash
    restart: unless-stopped
    volumes:
      - ./monitoring/logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./monitoring/logstash/config:/usr/share/logstash/config:ro
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - holidaibutler-network

# ==========================================
# VOLUMES
# ==========================================
volumes:
  mongodb_data:
    driver: local
  redis_data:
    driver: local
  backend_uploads:
    driver: local
  backend_logs:
    driver: local
  nginx_logs:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  elasticsearch_data:
    driver: local

# ==========================================
# NETWORKS
# ==========================================
networks:
  holidaibutler-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

---
# ==========================================
# BACKEND DOCKERFILE
# ==========================================
# backend/Dockerfile

# Multi-stage build for production optimization
FROM node:18.18.0-alpine AS base

# Install system dependencies
RUN apk add --no-cache \
    curl \
    git \
    python3 \
    make \
    g++ \
    && rm -rf /var/cache/apk/*

# Set working directory
WORKDIR /app

# Copy package files
COPY package*.json ./

# Development stage
FROM base AS development

# Install all dependencies (including dev)
RUN npm ci --include=dev

# Copy source code
COPY . .

# Create uploads and logs directories
RUN mkdir -p uploads logs

# Expose port and debug port
EXPOSE 3000 9229

# Start with nodemon for development
CMD ["npm", "run", "dev"]

# Production dependencies stage
FROM base AS production-deps

# Install only production dependencies
RUN npm ci --only=production && npm cache clean --force

# Production stage
FROM node:18.18.0-alpine AS production

# Install runtime dependencies
RUN apk add --no-cache curl && rm -rf /var/cache/apk/*

# Create app user
RUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001

# Set working directory
WORKDIR /app

# Copy production dependencies
COPY --from=production-deps --chown=nodejs:nodejs /app/node_modules ./node_modules

# Copy application code
COPY --chown=nodejs:nodejs . .

# Create directories with proper permissions
RUN mkdir -p uploads logs && chown -R nodejs:nodejs uploads logs

# Switch to non-root user
USER nodejs

# Expose port
EXPOSE 3000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:3000/api/health || exit 1

# Start application
CMD ["npm", "start"]

---
# ==========================================
# FRONTEND DOCKERFILE.DEV
# ==========================================
# frontend/Dockerfile.dev

FROM node:18.18.0-alpine

# Install system dependencies
RUN apk add --no-cache \
    curl \
    git \
    && rm -rf /var/cache/apk/*

# Install Expo CLI globally
RUN npm install -g @expo/cli@latest

# Set working directory
WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci

# Copy source code
COPY . .

# Expose Expo ports
EXPOSE 19000 19001 19002

# Start Expo development server
CMD ["npm", "start"]

---
# ==========================================
# BACKEND .DOCKERIGNORE
# ==========================================
# backend/.dockerignore

node_modules
npm-debug.log
.git
.gitignore
README.md
.env
.env.*
coverage
.nyc_output
logs
*.log
uploads
.vscode
.idea
*.swp
*.swo
*~
.DS_Store
Thumbs.db

---
# ==========================================
# FRONTEND .DOCKERIGNORE
# ==========================================
# frontend/.dockerignore

node_modules
.expo
.expo-shared
dist
web-build
ios
android
npm-debug.log
.git
.gitignore
README.md
.env
.env.*
.vscode
.idea
*.swp
*.swo
*~
.DS_Store
Thumbs.db

---
# ==========================================
# NGINX CONFIGURATION
# ==========================================
# nginx/nginx.conf

user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log notice;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging format
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # Basic settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 10M;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=auth:10m rate=5r/s;

    # Include server configurations
    include /etc/nginx/conf.d/*.conf;
}

---
# nginx/api.conf

upstream backend {
    server backend:3000;
    keepalive 32;
}

server {
    listen 80;
    server_name localhost api.holidaibutler.com;

    # Security headers
    add_header X-Content-Type-Options nosniff;
    add_header X-Frame-Options DENY;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;

    # Health check endpoint
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }

    # API routes
    location /api/ {
        # Rate limiting
        limit_req zone=api burst=20 nodelay;

        # Proxy settings
        proxy_pass http://backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
        proxy_read_timeout 86400;
    }

    # Authentication routes with stricter rate limiting
    location /api/auth/ {
        limit_req zone=auth burst=10 nodelay;

        proxy_pass http://backend;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # WebSocket support for real-time chat
    location /socket.io/ {
        proxy_pass http://backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_read_timeout 86400;
    }

    # Static file serving for uploads
    location /uploads/ {
        alias /app/uploads/;
        expires 30d;
        add_header Cache-Control "public, immutable";
    }

    # Monitoring endpoints
    location /metrics {
        proxy_pass http://backend;
        allow 172.20.0.0/16;  # Only allow from docker network
        deny all;
    }

    # Default location
    location / {
        return 404;
    }
}

---
# ==========================================
# REDIS CONFIGURATION
# ==========================================
# redis/redis.conf

# Network
bind 0.0.0.0
port 6379
protected-mode yes

# General
daemonize no
supervised no
pidfile /var/run/redis_6379.pid
loglevel notice
logfile ""
databases 16

# Memory
maxmemory 256mb
maxmemory-policy allkeys-lru

# Persistence
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir ./

# Append only file
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# Security
requirepass redis123

# Performance
tcp-keepalive 300
timeout 0

---
# ==========================================
# DEVELOPMENT DOCKER COMPOSE
# ==========================================
# docker-compose.dev.yml

version: '3.8'

services:
  mongodb:
    image: mongo:7.0
    container_name: holidaibutler-mongodb-dev
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password123
      MONGO_INITDB_DATABASE: holidaibutler_dev
    volumes:
      - mongodb_dev_data:/data/db
    networks:
      - dev-network

  redis:
    image: redis:7.2-alpine
    container_name: holidaibutler-redis-dev
    ports:
      - "6379:6379"
    command: redis-server --requirepass redis123
    volumes:
      - redis_dev_data:/data
    networks:
      - dev-network

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: development
    container_name: holidaibutler-backend-dev
    ports:
      - "3000:3000"
      - "9229:9229"
    environment:
      NODE_ENV: development
      MONGODB_URI: mongodb://admin:password123@mongodb:27017/holidaibutler_dev?authSource=admin
      REDIS_URL: redis://:redis123@redis:6379
      JWT_SECRET: dev-jwt-secret-key
    volumes:
      - ./backend:/app
      - /app/node_modules
    depends_on:
      - mongodb
      - redis
    networks:
      - dev-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: holidaibutler-frontend-dev
    ports:
      - "19000:19000"
      - "19001:19001"
      - "19002:19002"
    environment:
      EXPO_DEVTOOLS_LISTEN_ADDRESS: 0.0.0.0
    volumes:
      - ./frontend:/app
      - /app/node_modules
    networks:
      - dev-network

volumes:
  mongodb_dev_data:
  redis_dev_data:

networks:
  dev-network:
    driver: bridge

---
# ==========================================
# PRODUCTION DOCKER COMPOSE
# ==========================================
# docker-compose.prod.yml

version: '3.8'

services:
  mongodb:
    image: mongo:7.0
    container_name: holidaibutler-mongodb-prod
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_DB_NAME}
    volumes:
      - mongodb_prod_data:/data/db
      - ./backup:/backup
    networks:
      - prod-network

  redis:
    image: redis:7.2-alpine
    container_name: holidaibutler-redis-prod
    restart: always
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_prod_data:/data
    networks:
      - prod-network

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    container_name: holidaibutler-backend-prod
    restart: always
    environment:
      NODE_ENV: production
      MONGODB_URI: ${MONGODB_URI}
      REDIS_URL: ${REDIS_URL}
      JWT_SECRET: ${JWT_SECRET}
      CLAUDE_API_KEY: ${CLAUDE_API_KEY}
      STRIPE_SECRET_KEY: ${STRIPE_SECRET_KEY}
    volumes:
      - backend_uploads:/app/uploads
      - backend_logs:/app/logs
    depends_on:
      - mongodb
      - redis
    networks:
      - prod-network

  nginx:
    image: nginx:1.25-alpine
    container_name: holidaibutler-nginx-prod
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/api.conf:/etc/nginx/conf.d/default.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./certbot/conf:/etc/letsencrypt:ro
      - ./certbot/www:/var/www/certbot:ro
    depends_on:
      - backend
    networks:
      - prod-network

volumes:
  mongodb_prod_data:
  redis_prod_data:
  backend_uploads:
  backend_logs:

networks:
  prod-network:
    driver: bridge

---
# ==========================================
# SETUP SCRIPTS
# ==========================================
# scripts/docker-setup.sh

#!/bin/bash
set -e

echo "üê≥ Setting up HolidAIButler Docker environment..."

# Create necessary directories
mkdir -p nginx/ssl
mkdir -p monitoring/grafana/dashboards
mkdir -p monitoring/grafana/provisioning
mkdir -p monitoring/logstash/pipeline
mkdir -p backup
mkdir -p logs

# Set proper permissions
chmod 755 nginx/ssl
chmod -R 755 monitoring/

# Copy environment file
if [ ! -f .env ]; then
    cp .env.example .env
    echo "üìù Created .env file from template"
    echo "‚ö†Ô∏è  Please update .env with your actual values"
fi

# Build and start development environment
echo "üöÄ Starting development environment..."
docker-compose -f docker-compose.dev.yml up --build -d

echo "‚úÖ Docker setup complete!"
echo ""
echo "üìã Available services:"
echo "   Backend API: http://localhost:3000"
echo "   MongoDB: mongodb://localhost:27017"
echo "   Redis: redis://localhost:6379"
echo "   Frontend: http://localhost:19000"
echo ""
echo "üîß Development commands:"
echo "   docker-compose -f docker-compose.dev.yml logs -f    # View logs"
echo "   docker-compose -f docker-compose.dev.yml down       # Stop services"
echo "   docker-compose -f docker-compose.dev.yml restart    # Restart services"

---
# scripts/mongo-init.js

// MongoDB initialization script
db = db.getSiblingDB('holidaibutler');

// Create collections with validation
db.createCollection('users', {
  validator: {
    $jsonSchema: {
      bsonType: 'object',
      required: ['email', 'username', 'profile'],
      properties: {
        email: { bsonType: 'string' },
        username: { bsonType: 'string' },
        profile: { bsonType: 'object' }
      }
    }
  }
});

db.createCollection('pois', {
  validator: {
    $jsonSchema: {
      bsonType: 'object',
      required: ['name', 'location', 'category'],
      properties: {
        name: { bsonType: 'string' },
        location: { bsonType: 'object' },
        category: { bsonType: 'string' }
      }
    }
  }
});

// Create indexes
db.users.createIndex({ email: 1 }, { unique: true });
db.users.createIndex({ username: 1 }, { unique: true });
db.pois.createIndex({ location: '2dsphere' });
db.conversations.createIndex({ userId: 1, createdAt: -1 });
db.bookings.createIndex({ userId: 1, 'reservation.date': -1 });

print('‚úÖ MongoDB initialization complete');

---
# Makefile

.PHONY: help dev prod build test clean logs

help: ## Show this help message
	@echo 'Usage: make [target]'
	@echo ''
	@echo 'Targets:'
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "  %-15s %s\n", $$1, $$2}' $(MAKEFILE_LIST)

dev: ## Start development environment
	@echo "üöÄ Starting development environment..."
	docker-compose -f docker-compose.dev.yml up --build -d
	@echo "‚úÖ Development environment started!"

prod: ## Start production environment
	@echo "üöÄ Starting production environment..."
	docker-compose -f docker-compose.prod.yml up --build -d
	@echo "‚úÖ Production environment started!"

build: ## Build all images
	@echo "üî® Building images..."
	docker-compose build

test: ## Run tests
	@echo "üß™ Running tests..."
	docker-compose -f docker-compose.dev.yml exec backend npm test
	docker-compose -f docker-compose.dev.yml exec frontend npm test

clean: ## Clean up containers and volumes
	@echo "üßπ Cleaning up..."
	docker-compose -f docker-compose.dev.yml down -v
	docker-compose -f docker-compose.prod.yml down -v
	docker system prune -f

logs: ## Show logs
	@echo "üìã Showing logs..."
	docker-compose -f docker-compose.dev.yml logs -f

stop: ## Stop all services
	@echo "üõë Stopping services..."
	docker-compose -f docker-compose.dev.yml down
	docker-compose -f docker-compose.prod.yml down

restart: ## Restart all services
	@echo "üîÑ Restarting services..."
	docker-compose -f docker-compose.dev.yml restart

backup: ## Backup database
	@echo "üíæ Creating database backup..."
	docker-compose exec mongodb mongodump --out /backup/$(shell date +%Y%m%d_%H%M%S)

restore: ## Restore database (usage: make restore DATE=20231201_120000)
	@echo "üì• Restoring database from backup..."
	docker-compose exec mongodb mongorestore /backup/$(DATE)